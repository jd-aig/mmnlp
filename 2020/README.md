NLPCC Workshop on Multimodal Natural Language Processing 2020
=============================================================

Introduction

In recent years, significant breakthroughs have been made in artificial intelligence (AI) areas that involve single modality, such as speech, natural language, and vision. On many single-modality tasks such as object recognition, image generation, speech recognition, machine translation, AI systems have been performed at a level comparable to humans on specific data sets. These research results are also transformed into important applications in the real world. On the other hand, as the underlying problems of single-modality AI are getting solved, researchers realize that higher-level AI tasks often involve more complex information processing across multiple modalities. Meanwhile, research that limited to single modality often fail to take full advantages of cross-modal information. Therefore, it is of great interest to study multimodal modeling and learning approaches across more than one modality. The goal of this workshop is to provide a forum for the community to exchange ideas and latest advances for multimodal assisted natural language processing.

Invited Talks

Talk 1. Research Progress on Multimodal Summarization

Slides Link: Speaker-JiajunZhang.pdf

Abstract: 
Multimodality has become the main characteristics of news report and information exchange on the Internet. Multimodal automatic summarization aims to realize the process of automatically compressing and generating abstracts from multimodal information (such as text, audio, image and video) by using computer algorithms. It can be widely used in many fields such as news recommendation and e¬commerce service. This report will introduce the research progress of the methods and evaluation for multimodal summarization in recent years.

Speaker:  
Dr. Jiajun Zhang is research professor at Institute of Automation, Chinese Academy of Sciences. His research interests include machine translation and multi¬lingual natural language processing. 

Talk 2. Multimodal Pre¬training

Slides Link: Speaker-NanDuan.pdf

Abstract: 
In this talk, I will first review typical pre¬training models, and then introduce 3 latest multimodal pre¬trained models, including M3P for image ¬language tasks, Unicoder¬VL for video ¬language tasks and CodeBERT for code ¬language tasks. Last, I will discuss current issues and possible future directions.

Speaker: 
Dr. Nan Duan is a Principal Researcher & Research Manager at Microsoft Research Asia. He is an adjunct Professor of computer science at Tianjin University. His research interests include Question Answering, Semantic Parsing, Pre¬trained Models for Language + Knowledge/Image/Video/Programming Language, and Machine Reasoning. 


Talk 3. Artificial Intelligence of Art and Design

Abstract: 
Introduces the innovative application of artificial intelligence technology of computer science in the field of art and design. It includes the development and current situation of worldwide AI art and design fields, as well as the exploration achievements of Daozi AI system in Chinese painting creation, costume design, industrial design, installation art and other fields.

Speaker: 
Dr. Feng Gao is now a post¬doctoral research fellow at the Future Laboratory, Tsinghua University. His research interesting is to work on the intersection of computer science and Art, including but not limit on artificial intelligence and painting art, deep learning and painting robot, etc.

Talk 4. Multimodal intelligent analysis

Slides Link: Speaker-SiLiu.pdf

Abstract: 
Multimodal intelligent analysis is a hot topic in recent years. I will focus on the interaction of vision and language. First, I will talk about visual relationship detection, including human object interaction detection, human object relation segmentation and video relation detection. Then I will introduce referring expression and referring segmentation, which is important to intelligent robot and interactive image editing. Traditional referring expression methods mainly adopt two-¬stage architectures. The models are complex thus the inference speeds are low. In addition, the traditional referring segmentation methods focus on multimodal feature fusion and lack the ability of context modeling and reasoning with linguistic information. In this talk, I will introduce our recent solutions to the above problems.

Speaker:
Si Liu is an associate professor of Institute of Artificial Intelligence, Beihang University. Her research direction is cross-¬modal multimedia intelligent analysis, including natural language processing (NLP) and computer vision (CV). 

Talk 5. Multimodal Dialogue Systems in Ping An Life

Slides Link: Speaker- HaiqinYang.pdf 

Abstract:
Dialogue systems have been applied in various scenarios of Ping An Life to help agents selling insurance products and to engage customers in various services. In this talk, we will focus on our multimodal dialogue systems and share how they can be applied to reduce the operation cost while increasing users’ experience. We will outline the whole system architecture, share the core and newly proposed techniques and the real¬ world deployment. Some critical research problems will be discussed and presented.

Speaker:
Dr. Haiqin Yang is currently a senior research at Ping An Life Insurance Co. of China, Ltd., where he found the Statistical and Artificial Intelligence Learning (SAIL) Lab and provides two kinds of services, an online service of chitchat and an offline service of natural language generation. Currently, he is also an adjunct associate professor of the Chinese University of Hong Kong, Hong Kong. Dr. Yang’s research interests include machine learning and their applications to data analytics, including natural language processing and web mining, etc .


